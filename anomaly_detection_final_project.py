# -*- coding: utf-8 -*-
"""Anomaly Detection Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LWpE_m6rg2a2nkW5JkfJOoPnOfKt5Fhw

<font face="helvetica" color=#3c734d size=6>**Recognizing Credit Card Fraud using Unsupervised Anomaly Detection**</font><br>
Notebook Authored by: Katie McQuinn
<br>
<br>
<font face="helvetica" color=#3c734d size=5>**Background**</font><br>
Anomaly detection refers to the process of identifying rare items, events, or observations which differ significantly from the majority of the data. These “outliers” may indicate critical incidents, such as fraudulent activity, structural defects, or cyber attacks.

In the context of credit card transactions, anomalies typically correspond to behavior that deviates from typical spending patterns. Because fraud is rare and varied in behavior, supervised learning may not generalize well, especially when new types of fraud emerge. That's where unsupervised learning comes in.
<br>
<br>
<font face="helvetica" size=4> Why Unsupervised? </font></br>
* <b>Label Scarcity</b>: Real-world fraud datasets often lack accurate or complete labels.

* <b>Class Imbalance</b>: Fraudulent transactions are extremely rare (usually <1%).

* <b>Adaptability</b>: Unsupervised models can detect unknown or evolving fraud patterns.
<br>

<font face="helvetica" color=#3c734d size=5>**Dataset Description**</font><br>
* <b>Source:</b> [Kaggle Credit Card Fraud Dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data)<br>
* <b>Size:</b> 284,807 Transactions in .csv<br>
* <b>Features:</b> 30 columns(V1-V28 from PCA, ```Time```, ```Amount```, & ```Class```)<br>
* <b>Target (for val. only):</b>```Class``` (0=typical, 1=fraud)<br>
<br>

<font face="Helvetica" size=4 color=#3c734d> Import Packages </font></br>
"""

import pandas as pd
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (precision_score, recall_score, f1_score,
                             average_precision_score, confusion_matrix,
                             ConfusionMatrixDisplay, precision_recall_curve)
from sklearn.svm import OneClassSVM
from sklearn.ensemble import IsolationForest

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense
from tensorflow.keras.callbacks import EarlyStopping

"""<font face="helvetica" color=#3c734d size=5>**Exploratory Data Analysis**</font><br>

<font face="Helvetica" size=4 color=#3c734d> Load Data </font></br>
"""

fraud_df = pd.read_csv("/content/drive/MyDrive/Data_Science/DTSA 5510: Unsupervised Learning/Anomaly Detection/creditcard.csv")

fraud_df.head()

fraud_df.isnull().sum().max()

"""No null values.

<font face="Helvetica" size=4 color=#3c734d> Preprocess Data </font></br>
"""

scaler = StandardScaler()
fraud_df[["Time", "Amount"]] = scaler.fit_transform(fraud_df[["Time", "Amount"]])
X = fraud_df.drop(columns=["Class"])
y_true = fraud_df["Class"]
X_scaled = X.values

fraud_df.info()

fraud_df.describe()

fraud_df["Class"].value_counts()

"""Wildly imbalanced dataset! We will use some techniques later on in this project to address that.

<font face="Helvetica" size=4 color=#3c734d> Visualize Data</font></br>
"""

# Set up matplotlib style
sns.set(style="whitegrid")

# Distribution of Amount and Time (before scaling)
fig, axes = plt.subplots(1, 2, figsize=(14, 5))
sns.histplot(fraud_df["Amount"], bins=1000, ax=axes[0], kde=True)
axes[0].set_title("Distribution of Transaction Amount (Scaled)" )
axes[0].set_xlim(0, 10)
axes[0].set_ylim(0,20000)
sns.histplot(fraud_df["Time"], bins=100, ax=axes[1], kde=True)
axes[1].set_title("Distribution of Time (Scaled)")
plt.tight_layout()
plt.show()

"""Curious to see just how many rows have a transaction amount of 0 Dollars (1825). Code is below. It also looks like the vast majority of transactions are under $100.

As for the time distribution, it seems to be evenly distributed, i.e. no anomalous spikes in transactions.
"""

# Curious to see just how many rows have a transaction amount of $0
(fraud_df['Amount'] == 0).sum()

# Correlation heatmap
corr = fraud_df.drop(columns="Class").corr()
plt.figure(figsize=(12, 8))
sns.heatmap(corr, cmap="coolwarm", vmax=1.0, vmin=-1.0, square=True, cbar=True)
plt.title("Feature Correlation Heatmap (excluding Class)")
plt.show()

"""**Correlation Heatmap**<br>
* Features seem to be decorrelated (expected from PCA)
* Reinforces the fact that we'll need to use models that assume independence.
"""

# TSNE visualization on a 10K sample
sample = fraud_df.sample(10000, random_state=42)
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
tsne_result = tsne.fit_transform(StandardScaler().fit_transform(sample.drop(columns="Class")))
sample["tsne-1"], sample["tsne-2"] = tsne_result[:, 0], tsne_result[:, 1]
plt.figure(figsize=(10, 6))
sns.scatterplot(x="tsne-1", y="tsne-2", hue="Class", data=sample, palette="husl", alpha=0.6)
plt.title("t-SNE Projection (10K Sample)")
plt.grid(True)
plt.show()

"""Just a lil peek at distribution by class using t-SNE projection. This dataset will be good for anomaly detection because the fraud cases seem to cluster together. (Little blue spot in the center)

<font face="helvetica" color=#3c734d size=5>**Model Architecture**</font><br>
* We are going to use LSTM Autoencoder because it may be able to capture temporal dependencies
<br>
<br>

<font face="Helvetica" size=4 color=#3c734d> Model Evaluation Helper Function </font></br>

So that I can evaluate my models as we go.
"""

def evaluate_model(name, y_true, y_pred, score=None):
    print(f"Evaluation: {name}")
    print(f"Precision: {precision_score(y_true, y_pred):.4f}")
    print(f"Recall:    {recall_score(y_true, y_pred):.4f}")
    print(f"F1 Score:  {f1_score(y_true, y_pred):.4f}")
    if score is not None:
        print(f"AUPRC: {average_precision_score(y_true, score):.4f}")
    else:
        print(f"AUPRC: {average_precision_score(y_true, y_pred):.4f}")

"""<font face="Helvetica" size=4 color=#3c734d> Model Plot Helper Function </font></br>

So that I can evaluate my models as we go.
"""

def plot_model_results(y_true, score_dict, threshold_dict=None):
    """
    Plots precision-recall curves and prints AUPRC for multiple models.

    Parameters:
        y_true: array-like of true class labels (0 or 1)
        score_dict: dictionary with model name as key and anomaly scores as values
        threshold_dict: optional dictionary with thresholds used for binary predictions
    """
    plt.figure(figsize=(10, 7))

    for model_name, scores in score_dict.items():
        precision, recall, thresholds = precision_recall_curve(y_true, scores)
        ap = average_precision_score(y_true, scores)
        plt.plot(recall, precision, label=f"{model_name} (AUPRC = {ap:.4f})")

        if threshold_dict and model_name in threshold_dict:
            # Plot point on PR curve corresponding to chosen threshold
            bin_pred = (scores > threshold_dict[model_name]).astype(int)
            p = precision_score(y_true, bin_pred)
            r = recall_score(y_true, bin_pred)
            plt.scatter(r, p, marker='x', s=100, label=f"{model_name} @ threshold")

    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title("Precision-Recall Curves for Anomaly Detection Models")
    plt.legend(loc='best')
    plt.grid(True)
    plt.show()

"""<font face="Helvetica" size=4 color=#3c734d> Confusion Matrix Helper Function </font></br>

So that I can evaluate my models as we go.
"""

def plot_confusion_matrix(y_true, y_pred, model_name="Model", normalize=False):
    """
    Plots a confusion matrix for binary classification.

    Parameters:
        y_true (array-like): True binary labels (0 = normal, 1 = fraud)
        y_pred (array-like): Predicted binary labels
        model_name (str): Name of the model to display in the title
        normalize (bool): Whether to normalize counts to percentages
    """
    labels = ["Normal", "Fraud"]
    cm = confusion_matrix(y_true, y_pred, normalize="true" if normalize else None)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)

    plt.figure(figsize=(6, 5))
    disp.plot(cmap="Blues", values_format=".2f" if normalize else "d")
    plt.title(f"Confusion Matrix: {model_name}")
    plt.grid(False)
    plt.show()

"""<font face="helvetica" color=#3c734d size=5>**Model Building & Training**</font><br>

<font face="Helvetica" size=4 color=#3c734d> Build LSTM Autoencoder </font></br>
"""

def build_lstm_autoencoder(timesteps, n_features, latent_dim=16):
    inputs = Input(shape=(timesteps, n_features))
    x = LSTM(latent_dim, activation='relu')(inputs)
    x = RepeatVector(timesteps)(x)
    outputs = LSTM(n_features, activation='relu', return_sequences=True)(x)
    model = Model(inputs, outputs)
    model.compile(optimizer='adam', loss='mse')
    return model

"""<font face="Helvetica" size=4 color=#3c734d> Train LSTM Autoencoder </font></br>"""

epochs = 50
batch_size = 128
validation_split = 0.1

X_lstm = X_scaled.reshape(X_scaled.shape[0], 1, X_scaled.shape[1])

lstm_autoencoder = build_lstm_autoencoder(1, X_scaled.shape[1])
lstm_autoencoder.fit(X_lstm, X_lstm, epochs=epochs, batch_size=batch_size,
                     validation_split=validation_split, shuffle=True)

lstm_preds = lstm_autoencoder.predict(X_lstm)
lstm_errors = np.mean(np.square(X_lstm - lstm_preds), axis=(1, 2))
lstm_thresh = np.percentile(lstm_errors, 99.83)
fraud_df["lstm_pred"] = (lstm_errors > lstm_thresh).astype(int)

"""<font face="Helvetica" size=4 color=#3c734d> Evaluate LSTM Autoencoder </font></br>"""

evaluate_model("LSTM Autoencoder", y_true, fraud_df["lstm_pred"], lstm_errors)

plot_model_results(
    y_true=y_true,
    score_dict={"LSTM Autoencoder": lstm_errors},
    threshold_dict={"LSTM Autoencoder": lstm_thresh}
)

plot_confusion_matrix(y_true, fraud_df["lstm_pred"],
                      model_name="LSTM Autoencoder")

"""First go around has less-than-ideal scores! I'm going to try to utilize a dense autoencoder to try to get some better results.

<font face="Helvetica" size=4 color=#3c734d> Build Dense Autoencoder </font></br>
"""

def build_dense_autoencoder(input_dim):
    input_layer = Input(shape=(input_dim,))
    x = Dense(64, activation='relu')(input_layer)
    x = Dense(32, activation='relu')(x)
    x = Dense(16, activation='relu')(x)
    x = Dense(32, activation='relu')(x)
    x = Dense(64, activation='relu')(x)
    output_layer = Dense(input_dim, activation='linear')(x)
    model = Model(input_layer, output_layer)
    model.compile(optimizer='adam', loss='mse')
    return model

"""<font face="Helvetica" size=4 color=#3c734d> Train Dense Autoencoder </font></br>"""

epochs = 25
batch_size = 128
validation_split = 0.1

dense_autoencoder = build_dense_autoencoder(X_scaled.shape[1])
dense_autoencoder.fit(X_scaled, X_scaled, epochs=epochs, batch_size=batch_size,
                      validation_split=validation_split, shuffle=True)

dense_preds = dense_autoencoder.predict(X_scaled)
dense_errors = np.mean(np.square(X_scaled - dense_preds), axis=1)
dense_thresh = np.percentile(dense_errors, 99.83)
fraud_df["dense_pred"] = (dense_errors > dense_thresh).astype(int)

"""<font face="Helvetica" size=4 color=#3c734d> Evaluate Dense Autoencoder </font></br>"""

evaluate_model("Dense Autoencoder", y_true, fraud_df["dense_pred"], dense_errors)

plot_model_results(
    y_true=y_true,
    score_dict={"Dense Autoencoder": dense_errors},
    threshold_dict={"Dense Autoencoder": dense_thresh}
)

plot_confusion_matrix(y_true, fraud_df["dense_pred"],
                      model_name="Dense Autoencoder")

"""I will now up the epochs to 50 and add early stopping.

<font face="Helvetica" size=4 color=#3c734d> Re-train Dense Autoencoder </font></br>
"""

early_stopping = EarlyStopping(monitor='val_loss',
                               patience=7, restore_best_weights=True)

epochs = 50
batch_size = 128
validation_split = 0.1

dense_autoencoder = build_dense_autoencoder(X_scaled.shape[1])
dense_autoencoder.fit(X_scaled, X_scaled, epochs=epochs,
                      batch_size=batch_size,
                      validation_split=validation_split,
                      callbacks=[early_stopping],
                      shuffle=True)

dense_preds = dense_autoencoder.predict(X_scaled)
dense_errors = np.mean(np.square(X_scaled - dense_preds), axis=1)
dense_thresh = np.percentile(dense_errors, 99.83)
fraud_df["dense_pred"] = (dense_errors > dense_thresh).astype(int)

"""<font face="Helvetica" size=4 color=#3c734d> Re-evaluate Dense Autoencoder </font></br>"""

evaluate_model("Dense Autoencoder", y_true, fraud_df["dense_pred"], dense_errors)

plot_model_results(
    y_true=y_true,
    score_dict={"Dense Autoencoder": dense_errors},
    threshold_dict={"Dense Autoencoder": dense_thresh}
)

plot_confusion_matrix(y_true, fraud_df["dense_pred"],
                      model_name="Dense Autoencoder")

"""Somehow even worse scores.

<font face="Helvetica" size=4 color=#3c734d> Build One-Class SVM </font></br>
"""

svm = OneClassSVM(kernel='rbf', nu=0.0017, gamma=0.01)

"""<font face="Helvetica" size=4 color=#3c734d> Train One-Class SVM </font></br>"""

svm.fit(X_scaled)

svm_scores = -svm.decision_function(X_scaled)  # Higher score = more anomalous
svm_preds = (svm.predict(X_scaled) == -1).astype(int)  # -1 = anomaly → 1

"""<font face="Helvetica" size=4 color=#3c734d> Evaluate One-Class SVM </font></br>"""

evaluate_model("One-Class SVM", y_true, fraud_df["svm_pred"], score=svm_scores)

plot_model_results(
    y_true=y_true,
    score_dict={"One-Class SVM": svm_scores}
)

plot_confusion_matrix(
    y_true=y_true,
    y_pred=svm_preds,
    model_name="One-Class SVM"
)

"""This model performed worse than the other two. I'll do some hyperparameter tuning before moving on.

<font face="Helvetica" size=4 color=#3c734d> Re-Build One-Class SVM </font></br>

Just changed **gamma** from `0.01` -> `0.001`.
"""

svm = OneClassSVM(kernel='rbf', nu=0.0017, gamma=0.001)

"""<font face="Helvetica" size=4 color=#3c734d> Re-Train One-Class SVM </font></br>"""

svm.fit(X_scaled)

svm_scores = -svm.decision_function(X_scaled)  # Higher score = more anomalous
svm_preds = (svm.predict(X_scaled) == -1).astype(int)  # -1 = anomaly → 1

"""<font face="Helvetica" size=4 color=#3c734d> Re-Evaluate One-Class SVM </font></br>"""

evaluate_model("One-Class SVM", y_true, svm_preds, score=svm_scores)

plot_model_results(
    y_true=y_true,
    score_dict={"One-Class SVM": svm_scores}
)

plot_confusion_matrix(
    y_true=y_true,
    y_pred=svm_preds,
    model_name="One-Class SVM"
)

"""The One-Class SVM rebuild/retrain performed a little bit better but still not excellent. Let me try Isolation Forest.

<font face="Helvetica" size=4 color=#3c734d> Build Isolation Forest </font></br>
"""

n_estimators = 200
contamination = 0.0017
max_samples = 10000


iso_forest = IsolationForest(n_estimators=n_estimators,
                             contamination=contamination,
                             random_state=42,
                             max_samples=max_samples)

"""<font face="Helvetica" size=4 color=#3c734d> Train Isolation Forest </font></br>"""

iso_forest.fit(X_scaled)

iso_preds = iso_forest.predict(X_scaled)
fraud_df["iso_pred"] = (iso_preds == -1).astype(int)
fraud_df["iso_score"] = -iso_forest.decision_function(X_scaled)

"""<font face="Helvetica" size=4 color=#3c734d> Evaluate Isolation Forest </font></br>"""

evaluate_model("Isolation Forest", y_true, fraud_df["iso_pred"], fraud_df["iso_score"])

plot_model_results(
    y_true=y_true,
    score_dict={"Isolation Forest": fraud_df["iso_score"]}
)

plot_confusion_matrix(
    y_true=y_true,
    y_pred=fraud_df["iso_pred"],
    model_name="Isolation Tree"
)

"""While the isolation forest model doesn't seem to perform <i>excellently</i>, it has the highest AUPRC `0.2392` out of all four models that I tried. Let's see if we can make it even better.

<font face="Helvetica" size=4 color=#3c734d> Re-Build Isolation Forest (tuning n_estimators) </font></br>

Let's try increasing the `n_estimators` from `200` -> `300`
"""

n_estimators = 300
contamination = 0.0017
max_samples = 10000


iso_forest = IsolationForest(n_estimators=n_estimators,
                             contamination=contamination,
                             random_state=42,
                             max_samples=max_samples)

"""<font face="Helvetica" size=4 color=#3c734d> Re-Train Isolation Forest (tuning n_estimators)</font></br>"""

iso_forest.fit(X_scaled)

iso_preds = iso_forest.predict(X_scaled)
fraud_df["iso_pred"] = (iso_preds == -1).astype(int)
fraud_df["iso_score"] = -iso_forest.decision_function(X_scaled)

"""<font face="Helvetica" size=4 color=#3c734d> Re-Evaluate Isolation Forest (tuning n_estimators)</font></br>"""

evaluate_model("Isolation Forest", y_true, fraud_df["iso_pred"], fraud_df["iso_score"])

plot_model_results(
    y_true=y_true,
    score_dict={"Isolation Forest": fraud_df["iso_score"]}
)

plot_confusion_matrix(
    y_true=y_true,
    y_pred=fraud_df["iso_pred"],
    model_name="Isolation Tree"
)

"""When I increased the `n_estimators`, my `AUPRC` score went down a little bit. `0.2392`-> `0.2352`. Let's try again but instead, *decrease* the `n_estimators`

<font face="Helvetica" size=4 color=#3c734d> Re-Build Isolation Forest (tuning n_estimators)</font></br>

Let's try increasing the `n_estimators` from `200` -> `95`
"""

n_estimators = 95
contamination = 0.0017
max_samples = 10000


iso_forest = IsolationForest(n_estimators=n_estimators,
                             contamination=contamination,
                             random_state=42,
                             max_samples=max_samples)

"""<font face="Helvetica" size=4 color=#3c734d> Re-Train Isolation Forest (tuning n_estimators) </font></br>"""

iso_forest.fit(X_scaled)

iso_preds = iso_forest.predict(X_scaled)
fraud_df["iso_pred"] = (iso_preds == -1).astype(int)
fraud_df["iso_score"] = -iso_forest.decision_function(X_scaled)

"""<font face="Helvetica" size=4 color=#3c734d> Re-Evaluate Isolation Forest (tuning n_estimators)</font></br>"""

evaluate_model("Isolation Forest", y_true, fraud_df["iso_pred"], fraud_df["iso_score"])

plot_model_results(
    y_true=y_true,
    score_dict={"Isolation Forest": fraud_df["iso_score"]}
)

plot_confusion_matrix(
    y_true=y_true,
    y_pred=fraud_df["iso_pred"],
    model_name="Isolation Tree"
)

"""After decreasing the `n_estimators` from `200`-> `95`, we got a slight `AUPRC` score increase. `0.2392` -> `0.2595`. This is not a significant shift, but I'll stop here.

<font face="Helvetica" size=4 color=#3c734d> Re-Build Isolation Forest (tuning contamination)</font></br>

Let's try increasing the `contamination` from `0.0017` -> `0.003`
"""

n_estimators = 95
contamination = 0.003
max_samples = 10000


iso_forest = IsolationForest(n_estimators=n_estimators,
                             contamination=contamination,
                             random_state=42,
                             max_samples=max_samples)

"""<font face="Helvetica" size=4 color=#3c734d> Re-Train Isolation Forest </font></br>"""

iso_forest.fit(X_scaled)

iso_preds = iso_forest.predict(X_scaled)
fraud_df["iso_pred"] = (iso_preds == -1).astype(int)
fraud_df["iso_score"] = -iso_forest.decision_function(X_scaled)

"""<font face="Helvetica" size=4 color=#3c734d> Re-Evaluate Isolation Forest </font></br>"""

evaluate_model("Isolation Forest", y_true, fraud_df["iso_pred"], fraud_df["iso_score"])

plot_model_results(
    y_true=y_true,
    score_dict={"Isolation Forest": fraud_df["iso_score"]}
)

plot_confusion_matrix(
    y_true=y_true,
    y_pred=fraud_df["iso_pred"],
    model_name="Isolation Tree"
)

"""As you can see, by increasing the `contamination`, the recall score goes way up but the precision goes down. That is because we are widening the margin for what we consider a fraudulent transaction. There is an argument to be had about prioritizing recall over precision in cases like these. That is, catching *more* of the fraudulent cases means that you also will mark more legitimate purchase as fraud. The other option, by decreasing the `contamination` we will get fewer legitmate purchases classified as fraud but may end up missing much more of the fraudulent cases. This is an issue for a banking domain expert to decide!

For now, I will evaluate the best versions of each of my four models.

<font face="helvetica" color=#3c734d size=5>**Model Evaluation**</font><br>

<font face="Helvetica" size=4 color=#3c734d> AUPRC Evaluation Metrics </font></br>


I'm using Area Under the Precision-Recall Curve (AUPRC) to evaluate my models. This is a reasonable evaluation metric for this imbalanced dataset.
<br>
<br>
I will use these score scales as follows:
<br>

AUPRC Score|Details
----|----
**0.01-0.05**|catches more fraud than random (0.017)
**0.05-0.2** |solid score for unsupervised modeling
**0.2-0.5**|excellent for anomaly detection
**>0.5**|exceptional & rare
"""

evaluate_model("LSTM Autoencoder", y_true, fraud_df["lstm_pred"], lstm_errors)

evaluate_model("Dense Autoencoder", y_true, fraud_df["dense_pred"], dense_errors)

evaluate_model("One-Class SVM", y_true, fraud_df["svm_pred"])

evaluate_model("Isolation Forest", y_true, fraud_df["iso_pred"], fraud_df["iso_score"])

"""As you can see, the Isolation Forest performed the best out of all iterations of four models. The best scores of each model are below:

Model|AUPRC|Precision|Recall|F1
---|---|---|---|---
LSTM Autoencoder|0.1886|0.2433|0.2398|0.2416
Dense Autoencoder|0.0782|0.1464|0.1443|01453
One-Class SVM|0.0241|0.1431|0.1585|0.1504
Isolation Tree|**0.2595**|0.3093|0.3049|0.3071
"""



"""<font face="helvetica" color=#3c734d size=5>**Discussion**</font><br>

This project explored unsupervised anomaly detection techniques to detect fraudulent credit card transactions in a highly imbalanced dataset.
**Four** models were implemented:
* LSTM Autoencoder
* Dense Autoencoder
* One-Class SVM
* Isolation Forest

Given the nature of fraud detection — where labeled data is sparse and fraudulent behaviors constantly evolve — unsupervised methods offer a promising alternative to traditional supervised classification.

A key challenge of this project was the extreme class imbalance (only **0.17%** of transactions were fraud), which required evaluation metrics beyond standard accuracy.

**Area Under the Precision-Recall Curve (AUPRC)** was chosen as the primary metric, as it better reflects model performance under imbalance.

Among the four models:

* **The LSTM Autoencoder** was initially promising, capturing sequential transaction patterns, but required careful threshold tuning to balance precision and recall.

* **The Dense Autoencoder**, while faster to train, suffered from underfitting, even with added layers and early stopping.

* **The One-Class SVM** underperformed, likely due to poor scalability in high-dimensional space and sensitivity to hyperparameters.

* **The Isolation Forest**, a tree-based ensemble model that isolates anomalies through recursive partitioning, consistently outperformed the others. With tuned hyperparameters (`n_estimators=95`, `contamination=0.0017`), it achieved the best balance of precision and recall, with an AUPRC of `0.2595` — over 150x better than random guessing (baseline ≈ `0.0017`).

In tuning, it was observed that:

Increasing contamination improved recall at the cost of precision — a useful tradeoff when the goal is flagging as many frauds as possible. I still did not deem that my most successful model.

Decreasing n_estimators slightly improved AUPRC, possibly due to overfitting avoidance in deeper forests.

<font face="helvetica" color=#3c734d size=5>**Conclusion**</font><br>

Unsupervised anomaly detection offers a viable path for identifying credit card fraud when labeled data is limited or evolving. Through a combination of dimensionality-aware preprocessing, model-specific tuning, and appropriate evaluation metrics, the Isolation Forest model emerged as the most effective for this dataset.

However, no model is perfect. Isolation Forest missed many fraudulent transactions (`recall < 0.31`) and flagged some legitimate ones. This reflects a broader tension in fraud detection: catching more frauds vs. minimizing false positives — a business tradeoff that depends on the domain context.

In future work, combining models in an ensemble or incorporating semi-supervised methods (e.g. using a small labeled subset) could yield even better results. Additional features like user behavior or merchant patterns would also improve detection quality.

Overall, this project demonstrates that with thoughtful tuning and evaluation, unsupervised learning can meaningfully detect rare anomalies — and even outperform naive supervised baselines — in real-world fraud scenarios.
"""